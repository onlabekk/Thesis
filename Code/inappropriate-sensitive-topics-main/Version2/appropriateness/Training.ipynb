{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÐœÐ¾Ð´ÐµÐ»ÑŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'DeepPavlov/rubert-base-cased-conversational'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased-conversational and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ð”Ð°Ð½Ð½Ñ‹Ðµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df_val = pd.read_csv(\"val.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'inappropriate', 'offline_crime', 'online_crime', 'drugs',\n",
       "       'gambling', 'pornography', 'prostitution', 'slavery', 'suicide',\n",
       "       'terrorism', 'weapons', 'body_shaming', 'health_shaming', 'politics',\n",
       "       'racism', 'religion', 'sexual_minorities', 'sexism', 'social_injustice',\n",
       "       'human_labeled'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_val(val):\n",
    "    return round(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = 'inappropriate'\n",
    "threshold = 0\n",
    "df = df[(df[label_name] >= 1-threshold)|(df[label_name] <=threshold)]\n",
    "df_val = df_val[(df_val[label_name] >= 1-threshold)|(df_val[label_name] <=threshold)]\n",
    "df_test = df_test[(df_test[label_name] >= 1-threshold) | (df_test[label_name] <=threshold)]\n",
    "\n",
    "df[label_name] = df[label_name].apply(round_val)\n",
    "df_val[label_name] = df_val[label_name].apply(round_val)\n",
    "df_test[label_name] = df_test[label_name].apply(round_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'inappropriate', 'offline_crime', 'online_crime', 'drugs',\n",
       "       'gambling', 'pornography', 'prostitution', 'slavery', 'suicide',\n",
       "       'terrorism', 'weapons', 'body_shaming', 'health_shaming', 'politics',\n",
       "       'racism', 'religion', 'sexual_minorities', 'sexism', 'social_injustice',\n",
       "       'human_labeled'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnsafeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = UnsafeDataset(tokenizer(df.text.tolist(),\n",
    "                                        max_length=64,\n",
    "                                        truncation=True,\n",
    "                                        padding='longest'), df.inappropriate.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = UnsafeDataset(tokenizer(df_val.text.tolist(),\n",
    "                                       max_length=64,\n",
    "                                       truncation=True,\n",
    "                                       padding='longest'), df_val.inappropriate.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = UnsafeDataset(tokenizer(df_test.text.tolist(),\n",
    "                                       max_length=64,\n",
    "                                       truncation=True,\n",
    "                                       padding='longest'), df_test.inappropriate.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.file_utils import cached_property\n",
    "from typing import Tuple\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "class TrAr(TrainingArguments):\n",
    "    @cached_property\n",
    "    def _setup_devices(self) -> Tuple[\"torch.device\", int]:\n",
    "        return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(device)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert.parameters():\n",
    "    param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./unsafe/FINAL_VERS',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,            # total # of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=0,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=2500,\n",
    "    eval_steps=2500,\n",
    "    save_steps=2500,\n",
    "    evaluation_strategy='steps',metric_for_best_model = 'f1',greater_is_better = True, load_best_model_at_end = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,           # evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics  = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "trainer.add_callback(EarlyStoppingCallback(3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='12500' max='13270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/13270 36:41 < 02:15, 5.68 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.313800</td>\n",
       "      <td>0.274069</td>\n",
       "      <td>0.885798</td>\n",
       "      <td>0.882627</td>\n",
       "      <td>0.882859</td>\n",
       "      <td>0.885798</td>\n",
       "      <td>13.543100</td>\n",
       "      <td>782.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.162900</td>\n",
       "      <td>0.322487</td>\n",
       "      <td>0.888533</td>\n",
       "      <td>0.887054</td>\n",
       "      <td>0.886421</td>\n",
       "      <td>0.888533</td>\n",
       "      <td>13.531000</td>\n",
       "      <td>783.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.523052</td>\n",
       "      <td>0.883440</td>\n",
       "      <td>0.882210</td>\n",
       "      <td>0.881518</td>\n",
       "      <td>0.883440</td>\n",
       "      <td>13.541400</td>\n",
       "      <td>783.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.730064</td>\n",
       "      <td>0.885043</td>\n",
       "      <td>0.884201</td>\n",
       "      <td>0.883625</td>\n",
       "      <td>0.885043</td>\n",
       "      <td>13.560300</td>\n",
       "      <td>781.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.814693</td>\n",
       "      <td>0.885326</td>\n",
       "      <td>0.882978</td>\n",
       "      <td>0.882551</td>\n",
       "      <td>0.885326</td>\n",
       "      <td>13.551400</td>\n",
       "      <td>782.503000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12500, training_loss=0.12241329833984375, metrics={'train_runtime': 2201.8128, 'train_samples_per_second': 6.027, 'total_flos': 27311694989644800, 'epoch': 4.71})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='331' max='331' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [331/331 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.8845704002666753\n",
      "recall 0.8867960246095599\n",
      "fscore_weighted 0.8851992422915465\n",
      "roc_auc 0.935834400995755\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92      7839\n",
      "           1       0.81      0.74      0.77      2726\n",
      "\n",
      "    accuracy                           0.89     10565\n",
      "   macro avg       0.86      0.84      0.85     10565\n",
      "weighted avg       0.88      0.89      0.89     10565\n",
      "\n",
      "   threshold    f1  prec   rec   acc\n",
      "3        0.3  0.89  0.77  0.80  0.88\n",
      "4        0.4  0.89  0.79  0.77  0.89\n",
      "5        0.5  0.89  0.81  0.74  0.89\n",
      "2        0.2  0.88  0.74  0.82  0.88\n",
      "6        0.6  0.88  0.82  0.71  0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8851992422915465"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, roc_auc_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def get_metrics(preds):\n",
    "    preds, labels = preds.predictions, preds.label_ids\n",
    "    #standard round approach    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()    \n",
    "    pr, rec, f, _ = precision_recall_fscore_support(labels, pred_flat, average='weighted')  \n",
    "    \n",
    "    print(\"precision\", pr)\n",
    "    print(\"recall\", rec)\n",
    "    print(\"fscore_weighted\", f)\n",
    "    \n",
    "    #adjust threshold approach\n",
    "    preds_adj = np.array([[float(el1),float(el2)] for el1,el2 in preds])\n",
    "    preds_adj = softmax(preds_adj, axis = 1)\n",
    "    roc_auc = roc_auc_score(labels, preds_adj[:, 1])\n",
    "    print(\"roc_auc\", roc_auc)\n",
    "    \n",
    "    all_metrcis = []\n",
    "    for threshold in [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1]:\n",
    "        metrcis = []\n",
    "        pred_labels = (preds_adj[:, 1] >= threshold).astype(int)\n",
    "        metrcis.append(threshold)\n",
    "        metrcis.append(round(f1_score(labels, pred_labels, average='weighted'),2))  \n",
    "        metrcis.append(round(precision_score(labels, pred_labels),2))  \n",
    "        metrcis.append(round(recall_score(labels, pred_labels),2))  \n",
    "        metrcis.append(round(accuracy_score(labels, pred_labels),2))  \n",
    "        all_metrcis.append(metrcis)\n",
    "\n",
    "    df_metrics = pd.DataFrame(data = all_metrcis, columns = ['threshold','f1','prec','rec','acc'])\n",
    "    df_metrics = df_metrics.sort_values(by='f1', ascending=False)\n",
    "    \n",
    "    print(classification_report(labels, pred_flat))\n",
    "    \n",
    "    print(df_metrics.head())\n",
    "    \n",
    "    return f\n",
    "\n",
    "get_metrics(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
